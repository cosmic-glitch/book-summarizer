GPT4 is super expensive - 20X more.  So $1 of inference cost becomes $20.  Use GPT3-Turbo as much as possible.  
GPT3-Turbo worked well for chapter summarization only after I used one-shot prompting.

If you want to guide the form of the output, give a long concrete example to the LLM.  
The example itself could be one of the prior generations of the model that you liked.
So this way, an older output you liked becomes the new few shot prompt.

Anthropic models have strong hourly/daily rate-limits.  Often need to add "sleep" statements between calls to avoid hitting those.  
GPT models don't have such strong rate limits.

Using Anthropic models via Google Cloud improves the rate limits.  With GCP, there are no daily rate limits, and minute-level rate limits are really not
an issue for book summarization coz one can always sleep for a minute before the net generation.

Llama3 8B is surprisingly capable (>GPT3.5), but still not suitable for this project because of small context window size (8k).  
The cool thing is that the model can run locally on my macbook.  But the small context window limits what it can be used for.

Gemini 1.5 Pro is the GOAT when it comes to full book summarization!  Has a 1M token window so any book can be sent as part of a single input.